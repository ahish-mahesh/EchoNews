{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ahish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ahish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request,sys,time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "from math import log10\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 1...\n",
      "Done with page 1!\n",
      "Processing page 2...\n",
      "Done with page 2!\n",
      "Processing page 3...\n",
      "Done with page 3!\n",
      "Processing page 4...\n",
      "Done with page 4!\n",
      "Processing page 5...\n",
      "Done with page 5!\n",
      "Processing page 6...\n",
      "Done with page 6!\n",
      "Processing page 7...\n",
      "Done with page 7!\n",
      "Processing page 8...\n",
      "Done with page 8!\n",
      "Processing page 9...\n",
      "Done with page 9!\n",
      "Processing page 10...\n",
      "Done with page 10!\n"
     ]
    }
   ],
   "source": [
    "# Main logic\n",
    "\n",
    "#Total number of pages  to crawl\n",
    "pagesToGet = 10\n",
    "\n",
    "frame = []\n",
    "upperframe = []\n",
    "\n",
    "#The main loop where each page consists of the 30 latest articles\n",
    "for pageNo in range(1,pagesToGet+1):\n",
    "    \n",
    "    url = 'https://www.politifact.com/factchecks/list/?page='+str(pageNo)+'&ruling=true'\n",
    "    \n",
    "    #  Checking whether the link is valid or not\n",
    "    try:\n",
    "        page=requests.get(url)      \n",
    "    except Exception as e:\n",
    "        error_type, error_obj, error_info = sys.exc_info()      \n",
    "\n",
    "        print ('ERROR FOR LINK:',url)\n",
    "\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)\n",
    "        continue\n",
    "    \n",
    "    # Getting the entire page's content in html\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    # Retreiving only the links of articles from the page\n",
    "    links=soup.find_all('li',attrs={'class':'o-listicle__item'})\n",
    "\n",
    "    print(\"Processing page \"+str(pageNo)+\"...\")\n",
    "\n",
    "    #Navigating through each article link found in the current page\n",
    "    for j in links:\n",
    "#         Statement = j.find(\"div\",attrs={'class':'m-statement__quote'}).text.strip()\n",
    "        Link = \"https://www.politifact.com\"\n",
    "        Link += j.find(\"div\",attrs={'class':'m-statement__quote'}).find('a')['href'].strip()\n",
    "        \n",
    "        # For each article link, we crawl the entire article's page to get content\n",
    "        try:\n",
    "            page=requests.get(Link)      \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "        Title=soup.find('h2',attrs={'class':'c-title c-title--subline'}).text.strip()\n",
    "        ArticleTemp=soup.find('article',attrs={'class':'m-textblock'})\n",
    "        Article = soup.find_all('p')\n",
    "        fullArticle = []\n",
    "        \n",
    "        for i in range(2,len(Article)):\n",
    "            fullArticle.append(Article[i].text.strip())\n",
    "        fullArticle = \" \".join(fullArticle)\n",
    "        \n",
    "        Date = j.find('div',attrs={'class':'m-statement__body'}).find('footer').text.split(\"•\")[1]\n",
    "        Date = Date.replace(\"\\n\", \"\")\n",
    "        Source = j.find('div', attrs={'class':'m-statement__meta'}).find('a').text.strip()\n",
    "#         Label = j.find('div', attrs ={'class':'m-statement__content'}).find('img',attrs={'class':'c-image__original'}).get('alt').strip()\n",
    "        frame.append([Title, fullArticle, Link,Date,Source])\n",
    "    \n",
    "    print(\"Done with page \"+str(pageNo)+\"!\")\n",
    "    upperframe.extend(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Article</th>\n",
       "      <th>Link</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COVID-19 fatalities in the Rio Grande Valley d...</td>\n",
       "      <td>Health workers prepare to administer a COVID-1...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2020/oct...</td>\n",
       "      <td>October 16, 2020</td>\n",
       "      <td>Vicente Gonzalez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Roy Cooper compares Georgia's COVID-19 numbers...</td>\n",
       "      <td>North Carolina Gov. Roy Cooper speaks during a...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2020/oct...</td>\n",
       "      <td>October 15, 2020</td>\n",
       "      <td>Roy Cooper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fact-checking Jim Justice on West Virginia’s v...</td>\n",
       "      <td>West Virginia Gov. Jim Justice prepares for a ...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2020/oct...</td>\n",
       "      <td>October 15, 2020</td>\n",
       "      <td>Jim Justice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes, Wisconsin legislators haven’t passed a bi...</td>\n",
       "      <td>The Wisconsin Legislature last passed a bill i...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2020/oct...</td>\n",
       "      <td>October 7, 2020</td>\n",
       "      <td>Facebook posts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FBI director warned about white supremacist vi...</td>\n",
       "      <td>Democratic presidential candidate former Vice ...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2020/oct...</td>\n",
       "      <td>October 6, 2020</td>\n",
       "      <td>Joe Biden</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  COVID-19 fatalities in the Rio Grande Valley d...   \n",
       "1  Roy Cooper compares Georgia's COVID-19 numbers...   \n",
       "2  Fact-checking Jim Justice on West Virginia’s v...   \n",
       "3  Yes, Wisconsin legislators haven’t passed a bi...   \n",
       "4  FBI director warned about white supremacist vi...   \n",
       "\n",
       "                                             Article  \\\n",
       "0  Health workers prepare to administer a COVID-1...   \n",
       "1  North Carolina Gov. Roy Cooper speaks during a...   \n",
       "2  West Virginia Gov. Jim Justice prepares for a ...   \n",
       "3  The Wisconsin Legislature last passed a bill i...   \n",
       "4  Democratic presidential candidate former Vice ...   \n",
       "\n",
       "                                                Link               Date  \\\n",
       "0  https://www.politifact.com/factchecks/2020/oct...   October 16, 2020   \n",
       "1  https://www.politifact.com/factchecks/2020/oct...   October 15, 2020   \n",
       "2  https://www.politifact.com/factchecks/2020/oct...   October 15, 2020   \n",
       "3  https://www.politifact.com/factchecks/2020/oct...    October 7, 2020   \n",
       "4  https://www.politifact.com/factchecks/2020/oct...    October 6, 2020   \n",
       "\n",
       "             Source  \n",
       "0  Vicente Gonzalez  \n",
       "1        Roy Cooper  \n",
       "2       Jim Justice  \n",
       "3    Facebook posts  \n",
       "4         Joe Biden  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data containing Title, Article and other meta data\n",
    "data=pd.DataFrame(upperframe, columns=['Title', 'Article', 'Link','Date','Source'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1650"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming function\n",
    "def stemming(token):\n",
    "  if (token in set(nltk.corpus.stopwords.words('english'))):\n",
    "    return token\n",
    "  return nltk.stem.PorterStemmer().stem(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function\n",
    "def text_tokenizer(text):\n",
    "  tokens = nltk.word_tokenize(text)\n",
    "  stemmed_tokens = (stemming(token) for token in tokens) #stemming and removing stop words\n",
    "  return list([token for token in stemmed_tokens if token.isalnum()]) #removes tokens that are neither alphabetic characters nor digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Article is our main content. So indexing is done for them.\n",
    "corpus = data[\"Article\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer(stop_words = nltk.word_tokenize(' '.join(nltk.corpus.stopwords.words('english'))), tokenizer = text_tokenizer)\n",
    "tokenized_documents = count_vector.fit_transform(corpus)\n",
    "term_document_matrix = pd.DataFrame(tokenized_documents.toarray(), columns = count_vector.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index\n",
      "             Term  Doc_freq                                      Postings_List\n",
      "0              0         9     [80, 140, 230, 350, 500, 680, 890, 1130, 1400]\n",
      "1             07        10  [11, 41, 101, 191, 311, 461, 641, 851, 1091, 1...\n",
      "2              1       605  [2, 3, 4, 5, 7, 9, 11, 15, 16, 18, 23, 24, 26,...\n",
      "3             10       738  [2, 3, 6, 8, 9, 10, 12, 13, 15, 17, 18, 23, 24...\n",
      "4            100       207  [5, 16, 18, 19, 28, 35, 46, 48, 49, 58, 80, 84...\n",
      "5           1000         1                                             [1646]\n",
      "6            101         2                                       [1341, 1611]\n",
      "7           1029         4                            [815, 1025, 1265, 1535]\n",
      "8            103         2                                       [1320, 1590]\n",
      "9           1038        10   [3, 33, 93, 183, 303, 453, 633, 843, 1083, 1353]\n",
      "10          1040         1                                             [1647]\n",
      "11           108        15  [10, 40, 100, 190, 310, 460, 615, 640, 795, 85...\n",
      "12           109        18  [7, 37, 97, 153, 187, 243, 307, 363, 457, 513,...\n",
      "13         109th         1                                             [1639]\n",
      "14          10th        15  [169, 259, 276, 379, 396, 529, 546, 709, 726, ...\n",
      "15            11       436  [0, 2, 8, 17, 26, 29, 30, 32, 38, 47, 56, 59, ...\n",
      "16           110        19  [75, 84, 135, 144, 225, 234, 345, 354, 495, 50...\n",
      "17          1100      1650  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
      "18         110th         1                                             [1639]\n",
      "19           111        11  [441, 591, 610, 771, 790, 981, 1000, 1221, 124...\n",
      "20           112        12  [64, 124, 214, 334, 484, 664, 874, 1060, 1114,...\n",
      "21           113         7              [291, 411, 561, 741, 951, 1191, 1461]\n",
      "22         113th         1                                             [1639]\n",
      "23           114         6                   [441, 591, 771, 981, 1221, 1491]\n",
      "24          1151         5                       [615, 795, 1005, 1245, 1515]\n",
      "25         115th        24  [16, 46, 88, 106, 148, 196, 238, 316, 358, 466...\n",
      "26           116         7              [291, 411, 561, 741, 951, 1191, 1461]\n",
      "27         116th        15  [10, 40, 100, 190, 310, 460, 619, 640, 799, 85...\n",
      "28           117         6                   [441, 591, 771, 981, 1221, 1491]\n",
      "29         117th         3                                 [1077, 1317, 1587]\n",
      "...          ...       ...                                                ...\n",
      "8884          yi         1                                             [1622]\n",
      "8885       yield        18  [168, 258, 275, 378, 395, 528, 545, 708, 725, ...\n",
      "8886        ymca        11  [29, 59, 119, 209, 329, 479, 659, 869, 1109, 1...\n",
      "8887       yoken         8         [165, 255, 375, 525, 705, 915, 1155, 1425]\n",
      "8888     yonatan         9     [78, 138, 228, 348, 498, 678, 888, 1128, 1398]\n",
      "8889        york       544  [1, 10, 19, 24, 27, 31, 40, 49, 54, 57, 61, 62...\n",
      "8890      yorker        55  [70, 86, 87, 130, 146, 147, 166, 220, 236, 237...\n",
      "8891      yougov         9     [68, 128, 218, 338, 488, 668, 878, 1118, 1388]\n",
      "8892       young        84  [6, 11, 25, 29, 36, 41, 55, 59, 96, 101, 115, ...\n",
      "8893     younger        33  [6, 36, 96, 186, 280, 306, 400, 456, 550, 623,...\n",
      "8894    youngest         2                                       [1320, 1590]\n",
      "8895       youth        40  [29, 59, 119, 162, 209, 252, 281, 329, 372, 40...\n",
      "8896      youtub       133  [22, 23, 28, 29, 52, 53, 58, 59, 112, 113, 118...\n",
      "8897        zach         7              [285, 405, 555, 735, 945, 1185, 1455]\n",
      "8898     zachari         3                                 [1062, 1302, 1572]\n",
      "8899     zadroga         6                   [422, 572, 752, 962, 1202, 1472]\n",
      "8900     zakaria         2                                       [1328, 1598]\n",
      "8901  zalubowski         4                            [833, 1043, 1283, 1553]\n",
      "8902      zapata         6                   [421, 571, 751, 961, 1201, 1471]\n",
      "8903     zealand         8         [175, 265, 385, 535, 715, 925, 1165, 1435]\n",
      "8904        zero        69  [19, 49, 109, 172, 199, 262, 293, 319, 382, 41...\n",
      "8905       zhang         1                                             [1622]\n",
      "8906      zilker         2                                       [1329, 1599]\n",
      "8907     zimbabw         9     [79, 139, 229, 349, 499, 679, 889, 1129, 1399]\n",
      "8908      zimmer         1                                             [1648]\n",
      "8909        zimr         3                                 [1349, 1619, 1640]\n",
      "8910         zip         5                       [613, 793, 1003, 1243, 1513]\n",
      "8911        zone        26  [66, 126, 151, 216, 241, 336, 361, 486, 511, 6...\n",
      "8912        zoom         8         [174, 264, 384, 534, 714, 924, 1164, 1434]\n",
      "8913   zuckerman        10   [5, 35, 95, 185, 305, 455, 635, 845, 1085, 1355]\n",
      "\n",
      "[8914 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Indexing\n",
    "cols = sorted(list(term_document_matrix.columns))\n",
    "data = []\n",
    "for col in cols:\n",
    "  l = term_document_matrix[col].loc[term_document_matrix[col]>0].index.to_list()\n",
    "  data.append([col, len(l), l])\n",
    "inverted_index = pd.DataFrame(data ,columns=[\"Term\",\"Doc_freq\",\"Postings_List\"])\n",
    "print(\"Inverted index\\n\",inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
