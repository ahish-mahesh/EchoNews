{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/seshasaisuhashdesu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/seshasaisuhashdesu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request,sys,time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "from math import log10\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url of the page that we want to Scarpe\n",
    "#+str() is used to convert int datatype of the page no. and concatenate that to a URL for pagination purposes.\n",
    "page = 1\n",
    "url = 'https://www.politifact.com/factchecks/list/?page='+str(page)\n",
    "\n",
    "#Use the browser to get the URL. This is a suspicious command that might blow up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "     # this might throw an exception if something goes wrong.\n",
    "        page=requests.get(url)      \n",
    "    # this describes what to do if an exception is thrown \n",
    "except Exception as e:\n",
    "    error_type, error_obj, error_info = sys.exc_info()      \n",
    "    \n",
    "    #print the link that cause the problem\n",
    "    print ('ERROR FOR LINK:',url)\n",
    "    \n",
    "    #print error info and line that threw the exception                          \n",
    "    print (error_type, 'Line:', error_info.tb_lineno)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Scarpe it\n",
      "text/html; charset=utf-8\n"
     ]
    }
   ],
   "source": [
    "if \"Politifact\" in page.text:\n",
    "    print(\"Yes, Scarpe it\")\n",
    "\n",
    "print (page.headers.get(\"content-type\", \"unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 1...\n",
      "Processing page 2...\n",
      "Processing page 3...\n",
      "Processing page 4...\n",
      "Processing page 5...\n",
      "Processing page 6...\n",
      "Processing page 7...\n",
      "Processing page 8...\n",
      "Processing page 9...\n",
      "Processing page 10...\n",
      "Processing page 11...\n",
      "Processing page 12...\n",
      "Processing page 13...\n",
      "Processing page 14...\n",
      "Processing page 15...\n",
      "Processing page 16...\n",
      "Processing page 17...\n",
      "Processing page 18...\n",
      "Processing page 19...\n",
      "Processing page 20...\n"
     ]
    }
   ],
   "source": [
    "# Main logic\n",
    "\n",
    "#Total number of pages of the websites to crawl\n",
    "pagesToGet = 20\n",
    "\n",
    "frame = []\n",
    "upperframe = []\n",
    "\n",
    "for pageNo in range(1,pagesToGet+1):\n",
    "    \n",
    "    url = 'https://www.politifact.com/factchecks/list/?page='+str(pageNo)\n",
    "    \n",
    "    try:\n",
    "     # this might throw an exception if something goes wrong.\n",
    "        page=requests.get(url)      \n",
    "    # this describes what to do if an exception is thrown \n",
    "    except Exception as e:\n",
    "        error_type, error_obj, error_info = sys.exc_info()      \n",
    "\n",
    "        #print the link that cause the problem\n",
    "        print ('ERROR FOR LINK:',url)\n",
    "\n",
    "        #print error info and line that threw the exception                          \n",
    "        print (error_type, 'Line:', error_info.tb_lineno)\n",
    "        continue\n",
    "    \n",
    "#     time.sleep(2)  \n",
    "    \n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    links=soup.find_all('li',attrs={'class':'o-listicle__item'})\n",
    "\n",
    "    print(\"Processing page \"+str(pageNo)+\"...\")\n",
    "\n",
    "    #Segregating the contents found in the article\n",
    "    for j in links:\n",
    "        Statement = j.find(\"div\",attrs={'class':'m-statement__quote'}).text.strip()\n",
    "        Link = \"https://www.politifact.com\"\n",
    "        Link += j.find(\"div\",attrs={'class':'m-statement__quote'}).find('a')['href'].strip()\n",
    "        Date = j.find('div',attrs={'class':'m-statement__body'}).find('footer').text.split(\"•\")[1]\n",
    "        Date = Date.replace(\"\\n\", \"\")\n",
    "        Source = j.find('div', attrs={'class':'m-statement__meta'}).find('a').text.strip()\n",
    "        Label = j.find('div', attrs ={'class':'m-statement__content'}).find('img',attrs={'class':'c-image__original'}).get('alt').strip()\n",
    "        frame.append([Statement,Link,Date,Source,Label])\n",
    "    upperframe.extend(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>Link</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A photo shows a massive crowd at a recent Flor...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2020/oct...</td>\n",
       "      <td>October 18, 2020</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Says Nancy Pelosi was photographed with the Ma...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2020/oct...</td>\n",
       "      <td>October 18, 2020</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>pants-fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Says he “fully funded Missouri K-12 education ...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2020/oct...</td>\n",
       "      <td>October 16, 2020</td>\n",
       "      <td>Caleb Rowden</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Says the “The Rio Grande Valley is 4.7% of the...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2020/oct...</td>\n",
       "      <td>October 16, 2020</td>\n",
       "      <td>Vicente Gonzalez</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Says Kanye West got 40,000 votes in the presid...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2020/oct...</td>\n",
       "      <td>October 16, 2020</td>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Statement  \\\n",
       "0  A photo shows a massive crowd at a recent Flor...   \n",
       "1  Says Nancy Pelosi was photographed with the Ma...   \n",
       "2  Says he “fully funded Missouri K-12 education ...   \n",
       "3  Says the “The Rio Grande Valley is 4.7% of the...   \n",
       "4  Says Kanye West got 40,000 votes in the presid...   \n",
       "\n",
       "                                                Link               Date  \\\n",
       "0  https://www.politifact.com/factchecks/2020/oct...   October 18, 2020   \n",
       "1  https://www.politifact.com/factchecks/2020/oct...   October 18, 2020   \n",
       "2  https://www.politifact.com/factchecks/2020/oct...   October 16, 2020   \n",
       "3  https://www.politifact.com/factchecks/2020/oct...   October 16, 2020   \n",
       "4  https://www.politifact.com/factchecks/2020/oct...   October 16, 2020   \n",
       "\n",
       "             Source       Label  \n",
       "0    Facebook posts       false  \n",
       "1    Facebook posts  pants-fire  \n",
       "2      Caleb Rowden       false  \n",
       "3  Vicente Gonzalez        true  \n",
       "4   Instagram posts       false  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.DataFrame(upperframe, columns=['Statement','Link','Date','Source','Label'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6300"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(token):\n",
    "  if (token in set(nltk.corpus.stopwords.words('english'))):\n",
    "    return token\n",
    "  return nltk.stem.PorterStemmer().stem(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenizer(text):\n",
    "  tokens = nltk.word_tokenize(text)\n",
    "  stemmed_tokens = (stemming(token) for token in tokens) #stemming and removing stop words\n",
    "  return list([token for token in stemmed_tokens if token.isalnum()]) #removes tokens that are neither alphabetic characters nor digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data[\"Statement\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer(stop_words = nltk.word_tokenize(' '.join(nltk.corpus.stopwords.words('english'))), tokenizer = text_tokenizer)\n",
    "tokenized_documents = count_vector.fit_transform(corpus)\n",
    "term_document_matrix = pd.DataFrame(tokenized_documents.toarray(), columns = count_vector.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index\n",
      "             Term  Doc_freq                                      Postings_List\n",
      "0              1        36  [1062, 1302, 1572, 1872, 2202, 2334, 2562, 269...\n",
      "1             10        47  [812, 1022, 1070, 1262, 1310, 1329, 1532, 1580...\n",
      "2            100        50  [63, 123, 213, 333, 431, 483, 581, 663, 761, 8...\n",
      "3           1000         4                           [4572, 5082, 5622, 6192]\n",
      "4            102        12  [1324, 1594, 1894, 2224, 2584, 2974, 3394, 384...\n",
      "...          ...       ...                                                ...\n",
      "2092   zithromax         1                                             [6273]\n",
      "2093        zone        32  [1976, 2306, 2666, 2705, 2729, 3056, 3095, 311...\n",
      "2094        zoom        14  [826, 1036, 1276, 1546, 1846, 2176, 2536, 2926...\n",
      "2095         zti         2                                       [5679, 6249]\n",
      "2096  zuckerberg         1                                             [6275]\n",
      "\n",
      "[2097 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "cols = sorted(list(term_document_matrix.columns))\n",
    "data = []\n",
    "for col in cols:\n",
    "  l = term_document_matrix[col].loc[term_document_matrix[col]>0].index.to_list()\n",
    "  data.append([col, len(l), l])\n",
    "inverted_index = pd.DataFrame(data ,columns=[\"Term\",\"Doc_freq\",\"Postings_List\"])\n",
    "print(\"Inverted index\\n\",inverted_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
